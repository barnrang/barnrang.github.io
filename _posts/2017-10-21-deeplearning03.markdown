---
layout: post
title:  "Deep Learning CS231 Linear Classification"
date:   2017-10-21
categories: CS231
author: barnrang
series: cs231
chapter: 3
---
Deep learning 3 Linear classifier
สำหรับบทที่แล้วเราได้ศึกษาเกี่ยวกับ kNN -classifier ที่ไม่ค่อยจะแม่นยำนัก และที่สำคัญคือ ยิ่ง training data เพิ่มมากขึ้น เวลาที่ใช้ในการประเมินก็ยิ่งเพิ่มมากขึ้นเช่นกันซึ่งก็ไม่ใช่เรื่องดีสำหรับโจทย์ปัญหาที่ต้องใช้ข้อมูลเยอะๆ ซึ่งต่อไปเราจะเสนอ classification ที่ใช้เวลาคงที่ไม่ขึ้นกับจำนวน training data
<!--more-->
### Idea

หากรู้จัก SVM มาก่อนก็ดี แต่ถ้าไม่ก็ลองดูจากตัวอย่างต่อไปนี้ สมมุติเราค้องการแนกรูปคนเป็น ชาย หญิง (ไม่เอาเพศที่ 3 เด้อ) แล้วพล๊อตตำแหน่งของแต่ละจุดบนพิกัดมิติ(เอาเป็นว่าข้อมูลมีสิงมิติละกัน) เราสังเกตเห็นว่ากลุ่มข้อมูลชายหญิงจะถูกแบ่งออกเป็น 2 ส่วนได้ด้วยเส้นตรง ถ้าเขียนเป็นเว็กเตอร์ก็ $$ ax+by+c = 0$$ ถ้าเเทนค่า x,y แล้วมากกว่า 0 ก็เป็นเพศชายในขณะที่ถ้าน้อยกว่า 0 ก็เป็นเพศหญิง ถ้าเกิดเราแยกสมการออกเป็นสองอัน ให้ $$ S_b = ax+by+c $$ เป็นของผู้ชายและ $$ S_g = -ax-by-c $$ เป็นของผู้หญิง ถ้า $$S_b>S_g$$ แปลว่าเราทำนายข้อมูลของเราเป็น ผู้ชาย ในทางกลับกันถ้าทำนายว่าเป็นผู้หญิง $$S_g>0;S_b<0$$ ทำให้ $$S_g>S_b$$ ถ้าเรามีคลาสหลายๆอย่างเช่น แมว หมา ไก่ สิ่งที่เราจะทำคือเซตสมการเชิงเส้นสำหรับทุกคลาสซะสิ แล้วมาดูกันว่าคะแนนอันไหนเยอะสุด!

### Linear classification
ต่อไปนี้ เราจะใช้ $$N,D,C$$ แทน จำนวนข้อมูลเข้า, มิติของข้อมูล, และจำนวนคลาส(number, dimension, classes) แบบตัวอย่างเมื่อกี้ $$D=2, C=2$$ ในที่นี้เราให้ $$N=1$$ ไปก่อน หากสร้างสมการเหมือนตัวอย่างที่ผ่านมา จะได้ดังนี้
$$ S =
\begin{bmatrix}
x & y
\end{bmatrix}
\begin{bmatrix}
a & -a\\
b & -b
\end{bmatrix} +
\begin{bmatrix}
c & -c
\end{bmatrix} \\
S^T = \begin{bmatrix}
ax+by+c\\
-ax-by-c
\end{bmatrix}
$$

ทีนี้ถ้าให้ input ของเรามีมิติ 4 อินพุต 2 ตัวและมีคลาส 3 ($$D=4,N=2,C=3$$) ก็สามารถเขียนให้อยู่ในรูป
เมทริกซ์ได้ดังนี้

$$\begin{bmatrix}
x_{11} & x_{12} & x_{13} & x_{14}\\
x_{21} & x_{22} & x_{23} & x_{24}\\
\end{bmatrix}
\begin{bmatrix}
W_{11} & W_{12} & W_{13} \\
W_{21} & W_{22} & W_{23} \\
W_{31} & W_{32} & W_{33} \\
W_{41} & W_{42} & W_{43}
\end{bmatrix} +
\begin{bmatrix}
b_{11} & b_{12} & b_{13} \\
b_{21} & b_{22} & b_{23}
\end{bmatrix}
= S
$$

ในขณะที่

$$\begin{bmatrix}
b_{11} & b_{12} & b_{13}
\end{bmatrix} = \begin{bmatrix}
b_{21} & b_{22} & b_{23}
\end{bmatrix}
$$
{% highlight python %}
import numpy as np
N,D,C = 2,4,3
W = np.random.randn(D,C) #3x4
x = np.random.randn(N,D) #4x2
b = np.ones(C)
S = x.dot(W) + b
{% endhighlight %}
สังเกตเห็นได้ว่ามีติ `x.dot(W)` กับ `b`  ไม่เท่ากันแต่บวกกันได้โดยที่เราไม่จำเป็นต้องก๊อปปี้ b เพื่อสร้าง
เมทริกซ์ $$b$$ เหมือนในสมการ เราเรียกเทคนิคนี้ว่า
<a href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html" target="blank">Broadcasting</a>
### Loss function (error)

เนื่องจากเราไม่รู้ว่า model ของเรามันใช้ได้จริงหรือปล่าวนั้น เราจึงต้องเช็คความเอ๋อของ model แต่ไอ้ความเอ๋อนี่เราจะจัดการกับมันอย่างไรดีล่ะ?
ลองคิดซะว่าเรามี $$C=3$$ (หมู,หมา,แมว) แล้วคะแนนที่ได้ออกมาจากการทำนายคือ

$$xW =
\begin{bmatrix}
2.5 \\
-1.3 \\
1.2
\end{bmatrix}
$$

<In construction>
