---
layout: post
title:  "Deep Learning CS231 Linear Classification"
date:   2017-10-21
categories: CS231
author: barnrang
series: cs231
chapter: 3
---
Deep learning 3 Linear classifier
สำหรับบทที่แล้วเราได้ศึกษาเกี่ยวกับ kNN -classifier ที่ไม่ค่อยจะแม่นยำนัก และที่สำคัญคือ ยิ่ง training data เพิ่มมากขึ้น เวลาที่ใช้ในการประเมินก็ยิ่งเพิ่มมากขึ้นเช่นกันซึ่งก็ไม่ใช่เรื่องดีสำหรับโจทย์ปัญหาที่ต้องใช้ข้อมูลเยอะๆ ซึ่งต่อไปเราจะเสนอ classification ที่ใช้เวลาคงที่ไม่ขึ้นกับจำนวน training data
<!--more-->
### Idea
![SVNExample]({{"/assets/images/SVMEX.jpg" | absolute_url}})

หากรู้จัก SVM มาก่อนก็ดี แต่ถ้าไม่ก็ลองดูจากตัวอย่างต่อไปนี้ สมมุติเราค้องการแนกรูปคนเป็น ชาย หญิง (ไม่เอาเพศที่ 3 เด้อ) แล้วพล๊อตตำแหน่งของแต่ละจุดบนพิกัดมิติ(เอาเป็นว่าข้อมูลมีสิงมิติละกัน) เราสังเกตเห็นว่ากลุ่มข้อมูลชายหญิงจะถูกแบ่งออกเป็น 2 ส่วนได้ด้วยเส้นตรง ถ้าเขียนเป็นเว็กเตอร์ก็ $$ ax+by+c = 0$$ ถ้าเเทนค่า x,y แล้วมากกว่า 0 ก็เป็นเพศชายในขณะที่ถ้าน้อยกว่า 0 ก็เป็นเพศหญิง ถ้าเกิดเราแยกสมการออกเป็นสองอัน ให้ $$ S_b = ax+by+c $$ เป็นของผู้ชายและ $$ S_g = -ax-by-c $$ เป็นของผู้หญิง ถ้า $$S_b>S_g$$ แปลว่าเราทำนายข้อมูลของเราเป็น ผู้ชาย ในทางกลับกันถ้าทำนายว่าเป็นผู้หญิง $$S_g>0;S_b<0$$ ทำให้ $$S_g>S_b$$ ถ้าเรามีคลาสหลายๆอย่างเช่น แมว หมา ไก่ สิ่งที่เราจะทำคือเซตสมการเชิงเส้นสำหรับทุกคลาสซะสิ แล้วมาดูกันว่าคะแนนอันไหนเยอะสุด!

### Linear classification
ต่อไปนี้ เราจะใช้ $$N,D,C$$ แทน จำนวนข้อมูลเข้า, มิติของข้อมูล, และจำนวนคลาส(number, dimension, classes) แบบตัวอย่างเมื่อกี้ $$D=2, C=2$$ ในที่นี้เราให้ $$N=1$$ ไปก่อน หากสร้างสมการเหมือนตัวอย่างที่ผ่านมา จะได้ดังนี้
$$ S =
\begin{bmatrix}
x & y
\end{bmatrix}
\begin{bmatrix}
a & -a\\
b & -b
\end{bmatrix} +
\begin{bmatrix}
c & -c
\end{bmatrix} \\
S^T = \begin{bmatrix}
ax+by+c\\
-ax-by-c
\end{bmatrix}
$$

ทีนี้ถ้าให้ input ของเรามีมิติ 4 อินพุต 2 ตัวและมีคลาส 3 ($$D=4,N=2,C=3$$) ก็สามารถเขียนให้อยู่ในรูป
เมทริกซ์ได้ดังนี้

$$\begin{bmatrix}
x_{11} & x_{12} & x_{13} & x_{14}\\
x_{21} & x_{22} & x_{23} & x_{24}\\
\end{bmatrix}
\begin{bmatrix}
W_{11} & W_{12} & W_{13} \\
W_{21} & W_{22} & W_{23} \\
W_{31} & W_{32} & W_{33} \\
W_{41} & W_{42} & W_{43}
\end{bmatrix} +
\begin{bmatrix}
b_{11} & b_{12} & b_{13} \\
b_{21} & b_{22} & b_{23}
\end{bmatrix}
= S
$$

ในขณะที่

$$\begin{bmatrix}
b_{11} & b_{12} & b_{13}
\end{bmatrix} = \begin{bmatrix}
b_{21} & b_{22} & b_{23}
\end{bmatrix}
$$
{% highlight python %}
import numpy as np
N,D,C = 2,4,3
W = np.random.randn(D,C) #3x4
x = np.random.randn(N,D) #4x2
b = np.ones(C)
S = x.dot(W) + b
{% endhighlight %}
สังเกตเห็นได้ว่ามีติ `x.dot(W)` กับ `b`  ไม่เท่ากันแต่บวกกันได้โดยที่เราไม่จำเป็นต้องก๊อปปี้ b เพื่อสร้าง
เมทริกซ์ $$b$$ เหมือนในสมการ เราเรียกเทคนิคนี้ว่า
<a href="https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html" target="blank">Broadcasting</a>
### Loss function (error)

เนื่องจากเราไม่รู้ว่า model ของเรามันใช้ได้จริงหรือปล่าวนั้น เราจึงต้องเช็คความเอ๋อของ model แต่ไอ้ความเอ๋อนี่เราจะจัดการกับมันอย่างไรดีล่ะ?
ลองคิดซะว่าเรามี $$C=3,N=1$$ (หมู,หมา,แมว) แล้วคะแนนที่ได้ออกมาจากการทำนายคือ

$$xW + b=
\begin{bmatrix}
2.5 \\
-1.3 \\
2.1
\end{bmatrix}
$$

สมมติให้ผลจริงเป็นหมู model ของเราก็สามารถทำนายผลได้ถูกต้องซึ่งอาจดูเหมือนไม่มีจุดเอ๋อใดๆ แต่หากดูให้ดีๆจะพบว่า

$$xW + b =
\begin{bmatrix}
\color{green}{2.5} \\
\color{green}{-1.3}\\
\color{red}{2.1}
\end{bmatrix}
$$

คะแนน $$2.1$$ ถึงไม่ใช่ตัวที่สูงที่สุดแต่ก็ถือว่ามากจนน่ากลัว (ถ้าโชคไม่ดีก็อาจเอาชนะ class หมูได้) เราก็ควร
ที่จะประเมินว่า model เรายังมีจุดบกพร่อง

หากเราลองเปรียบเทียบกับคะแนนของคลาสที่ถูกต้อง จะได้ว่าผลต่างคือ $$2.1-2.5 = -0.4$$ ซึ่งถือว่าไม่มาก
หรือเราสามารถบอกได้ว่า $$-0.4$$ ยังมีนัยยะสำคัญที่บอกได้ว่า model ยังบกพร่องอยู่ซึ่งนำพามาสู่การกำหนดขอบเขตว่าเท่าไหร่ถึงจะนับว่าบกพร่องอยู่ โดยในที่นี้ขอใช้สัญลักษณ์ $$\Delta$$ (Margin)

คนส่วนใหญ่มักกำหนดให้ $$\Delta$$ เท่ากับ 1 หลังจากกำหนดเสร็จเราก็สามารถระบุได้ชัดเจนยิ่งขึ้นว่าค่าที่ได้ออกมามันบกพร่องอยู่เท่าไหร่ วิธีการที่ง่ายที่สุดและนิยมคือบวกด้วย $$\Delta$$ เข้าไปเลย จึงได้ว่าค่าความบกพร่องคือ
$$-0.4 + \Delta = -0.4 + 1 = 0.6$$

ณ ที่นี้ขอกำหนดนิยามคำว่า loss หรือ "ค่าสูญเสีย" ซึ่งพูดสั้นๆได้ว่าเป็นค่าที่
แสดงถึงความไม่แม่นยำหรือยังบกพร่องอยู่ของ model โดยต่อไปจะใช้สัญลักษณ์ $$L$$ และเราจะทำการเทียบ
สัมประสิทธิ์ของเมทริกซ์คะแนนดังนี้

$$xW + b =
\begin{bmatrix}
\color{green}{2.5} \\
\color{green}{-1.3}\\
\color{red}{2.1}
\end{bmatrix} =
\begin{bmatrix}
s_1 \\
s_2\\
s_3
\end{bmatrix}
$$

และ $$y$$ แทนคลาสที่ถูกต้อง (ในที่นี้ $$y=1$$) จากข้อความข้างต้น เราสามารถสรุป loss ที่ class ใดๆได้ว่า

$$L_j = max(0, s_j - s_y + \Delta)$$

และหากรวมของทุกคลาสเข้าด้วยกัน

$$L = \sum_{j\neq y}max(0, s_j - s_y + \Delta) $$

สำหรับกรณีนี้ หากลองกระจายออกมา

$$L = max(0, -1.3-2.5+1) + max(0, 2.1-2.5+1) = max(0,-2.8) + max(0,0.6) = 0.6$$

หากเปลี่ยน class ที่ถูกต้องเป็นหมา ($$y=2$$) ในกรณีนี้ model ทำนายผลได้ไม่ดีนัก

$$L = max(0, 2.5-(-1.3)+1) + max(0, 2.1-(-1.3)+1) = max(0,4.8) + max(0,4.4) = 9.2$$

จะเห็นว่า loss พุ่งกระฉูดเมื่อการทำนายผิดพลาด

### General Formula
สำหรับบทสั้นๆที่นี้ เราจะมาทำให้ loss ของเราอยู่ในสมการสำหรับ $$N,C,D$$ ใดๆและตัวแปรดังต่อไปนี้

- $$y_i$$ class ที่ถูกต้องสำหรับ input ตัวที่ i ($$1\leq i \leq N$$)
- $$s_{ij}$$ score สำหรับ input ตัวที่ i class ที่ j ($$1\leq j\leq C$$)

$$\begin{bmatrix}
x_{11} & x_{12} & x_{13} & x_{14}\\
x_{21} & x_{22} & x_{23} & x_{24}\\
\end{bmatrix}
\begin{bmatrix}
W_{11} & W_{12} & W_{13} \\
W_{21} & W_{22} & W_{23} \\
W_{31} & W_{32} & W_{33} \\
W_{41} & W_{42} & W_{43}
\end{bmatrix} +
\begin{bmatrix}
b_{11} & b_{12} & b_{13} \\
b_{21} & b_{22} & b_{23}
\end{bmatrix}
= \begin{bmatrix}
s_{11} & s_{12} & s_{13} \\
s_{21} & s_{22} & s_{23}
\end{bmatrix}
$$

$$L_i = \sum_{j\neq y_i}max(0, s_{ij} - s_{iy_i} + \Delta)$$

$$L = \sum_{i=1}^{N}L_i = \sum_{i}\sum_{j\neq y_i}max(0, s_{ij} - s_{iy_i} + \Delta)$$

แน่นอนว่าไอ้ "Loss" เนี่ยมันก็ไม่ได้มีแบบเดียวหรอก เอาจริงๆที่คลาสิกสุดก็
<a href="https://en.wikipedia.org/wiki/Least_squares" target="blank">Least Square</a>
 หรือที่เอาไว้คาดคะเนกราฟเชิงเส้น ส่วนที่ได้แนะนำไปนั้นเขาเรียกว่า SVM hinge loss ซึ่งเป็น loss ประเภทหนึ่งที่
 ใช้ได้ดีสำหรับโจทย์แยกประเภท สำหรับครั้งต่อไปเราจะลองตะลุย loss ที่มีนามว่า Softmax (ไม่มีความเกี่ยวข้องกับ
Baymax) ซึ่งเป็นคู่แข่งที่ดีกับ SVM hinge loss และก็การใช้ประโยชน์จาก loss กรุณาติดตามตอนต่อไป
